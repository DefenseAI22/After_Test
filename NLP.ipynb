{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d4a079-9441-4df3-ad71-6975b607a342",
   "metadata": {},
   "source": [
    "### 1) 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99367428-76bf-4ad5-b74b-cc6cf4d96fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shinjoohwan/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, pandas as pd, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          get_scheduler)          \n",
    "from torch.optim import AdamW                   \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ec6250-fa0f-40d5-ba64-1506dab13188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db1538-e4f6-42f7-95db-eacafedbd371",
   "metadata": {},
   "source": [
    "### 2) 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd6ae69-2845-42fb-bf0e-df18824a4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"NLPtrain.csv\"\n",
    "test_path  = \"NLPtest.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42f9be-d111-4a5c-950b-3f0f2522f287",
   "metadata": {},
   "source": [
    "### 3) Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6f70af3-769f-45b5-8fcb-f6cf7e0853bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() # LabelEncoder 객체 생성(0 ~ n_classes-1 사이의 정수로 매핑)\n",
    "train_df[\"label_id\"] = le.fit_transform(train_df[\"label\"]) # fit: trian_df[\"label\"] 열을 스캔해 고유 레이블 목록(le.classes_)을 학습 / transform: 학습된 순서에 따라 각 레이블을 정수로 변환\n",
    "NUM_LABELS = len(le.classes_) # le.classes: 식별된 고유 레이블 배열 -> 클래스 개수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2005bc-15fc-4967-b289-a6964589ed93",
   "metadata": {},
   "source": [
    "### 4) Train / Valudation (라벨 분포 유지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d81efc0-1b49-4ce0-bd8d-6cf8f4f11d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    stratify=train_df[\"label_id\"],\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7f569-4e7b-42fb-a401-3dec2f360302",
   "metadata": {},
   "source": [
    "### 5) 토크나이저 & 동적 MAX_LEN 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f76e933-4271-4b2a-a3cc-8841b344b478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "길이 측정: 100%|███████████████████████████| 200/200 [00:00<00:00, 26250.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동적으로 산출된 MAX_LEN = 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-base-multilingual-cased\" # 다국어 대소문자 구분 BERT\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# AutoTokenizer는 모델 이름만 주면 자동으로 맞춤 토크나이저 클래스 불러옴\n",
    "\n",
    "def calc_dynamic_max_len(texts, percentile=95, hard_cap=512): # texts: 학습, 추론에 사용할 문장 리스트 / percentile: 몇 퍼센트 지점까지 커버할지 결정 / hard_cap: BERT 구조적 한계(512 토큰) 넘지 않도록 상한선 \n",
    "    # tokenizer.encode(): 문장 t를 토큰 ID 시퀀스로 변환\n",
    "    # add_special_tokens=TRUE: 토크나이저가 모델에 필요한 '특수 토큰'을 자동으로 앞.뒤에 붙여서 인코딩하도록 지시하는 매개변수\n",
    "    lengths = [len(tokenizer.encode(t, add_special_tokens=True))\n",
    "               for t in tqdm(texts, desc=\"길이 측정\")]\n",
    "    dyn_len = int(np.percentile(lengths, percentile)) # lenghts 리스트(각 문장의 토큰 수 모음)의 분포를 통계적으로 살펴보고, 지정한 분위값에 해당하는 길이를 구하는 과정\n",
    "    return min(dyn_len, hard_cap)\n",
    "\n",
    "MAX_LEN = calc_dynamic_max_len(train_df[\"data\"], percentile=95)\n",
    "print(f\"동적으로 산출된 MAX_LEN = {MAX_LEN}\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f08d1-cea7-4dc5-9c9d-6b36d27d4929",
   "metadata": {},
   "source": [
    "### 6) PyTorch Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "881791bf-21c4-4df2-b144-def68ee19570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset): # 텍스트를 토큰화해 텐서로 변환, (학습 단계라면) 레이블도 함께 반환하도록 설계\n",
    "    def __init__(self, df, training=True):\n",
    "        self.texts     = df[\"data\"].tolist() # 문장 리스트를 미리 추출해 메모리 상에 보관\n",
    "        self.labels    = df[\"label_id\"].tolist() if training else None # # 학습 모드일 때만 레이블 리스트를 저장\n",
    "        self.training  = training # 학습 모드 여부\n",
    "        \n",
    "    def __len__(self):  \n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # 텍스트 토큰화\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True, # MAX_LEN보다 길면 자름\n",
    "            padding=\"max_length\", # 빈 칸을 PAD 토큰으로 채움\n",
    "            max_length=MAX_LEN, # 앞서 계산한 동적 최대 길이\n",
    "            return_tensors=\"pt\", # PyTorch 텐서 형태로 반환\n",
    "        )\n",
    "        # tokenizer에서 return_tensors='pt'를 쓰면 첫 차원(batch)=1인 텐서가 생성\n",
    "        # squeeze(0)으로 불필요한 차원을 제거해 MAX_LEN(1D) 텐서로 변환\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        if self.training:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long) # 레이블을 torch.long형 텐서로 변환 후 item 딕셔너리에 추가\n",
    "        return item\n",
    "\n",
    "train_ds = TextDataset(train_df)\n",
    "val_ds   = TextDataset(val_df)\n",
    "test_ds  = TextDataset(test_df, training=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39baa0-5a71-4f44-932d-0001f7181d0b",
   "metadata": {},
   "source": [
    "### 7) 모델, 옵티마이저, 스케줄러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17f468c5-34b5-47b1-ac04-e3d9d9c5a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# AutoModelForSequenceClassification: 허깅페이스가 \"시퀀스 분류용\" 헤드를 자동으로 붙인 래퍼 클래스\n",
    "# 사전 학습 가중치를 불러오면서, 마지막 분류 레이의 출력 뉴런 수를 NUM_LABELS개로 교체\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # model.parameters(): 학습 대상 파라미터 전부 전달\n",
    "total_steps  = len(train_loader) * EPOCHS # 전체 학습 과정에서 optimizer가 파라미터를 업데이트하는 총 횟수\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", # 선형 감소 스케줄 선택 / 훈련이 진행됨에 따라 학습률이 초기값 -> 0으로 직선형으로 떨어짐\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps), # 전체 스텝의 10%를 워밍업으로 지정. 워밍업 동안 학습률이 0 -> 초기값까지 점진적으로 상승\n",
    "    num_training_steps=total_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eeecf4-ece1-4052-8cf8-e426dc91f4f7",
   "metadata": {},
   "source": [
    "### 8) 학습 & 검증(Early-Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9c4ebdc-8484-41a8-aaef-18fa6ac73755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train 0.7001 | Val 0.6703 | Acc 0.6275\n",
      "모델 저장 (acc=0.6275)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train 0.6156 | Val 0.5433 | Acc 0.7843\n",
      "모델 저장 (acc=0.7843)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train 0.5036 | Val 0.4231 | Acc 0.8235\n",
      "모델 저장 (acc=0.8235)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train 0.2865 | Val 0.4888 | Acc 0.7647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train 0.1622 | Val 0.3906 | Acc 0.8627\n",
      "모델 저장 (acc=0.8627)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train 0.0852 | Val 0.5299 | Acc 0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train 0.0344 | Val 0.4936 | Acc 0.8627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train 0.0201 | Val 0.6788 | Acc 0.8235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train 0.0067 | Val 0.6360 | Acc 0.8235\n",
      "Early stopping\n",
      "최고 검증 정확도: 0.8627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "best_acc = 0.0 # 최고 검증 정확도\n",
    "patience = 4 # Early Stopping 허용 횟수\n",
    "pat_cnt =  0 # 최근 성능 개선 없었던 연속 epoch 수\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ----- Train -----\n",
    "    model.train()\n",
    "    train_loss = 0.0 # epoch별 평균 Loss 계산용 누적 변수\n",
    "    for batch in tqdm(train_loader, desc=f\"[Epoch {epoch}] Train\", leave=False):  # train_loader에서 배치를 하나씩 뽑아 온다\n",
    "        batch = {k: v.to(device) for k, v in batch.items()} # 데이터를 디바이스로 이동\n",
    "        outputs = model(**batch) # 순전파 /  **: 딕셔너리 언패킹 - 딕셔너리의 각 키 - 값 쌍을 키워드 인자로 풀어서 함수에 전달(키워드 인자: 파이썬 함수에 \"인자 이름 = 값\" 형태로 전달되는 매개변수)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 그래디언트 클리핑: 역전파로 계산된 param.grad의 L2 노름이 1.0보다 크면 비율을 맞춰 축소 -> 너무 큰 그래디언트가 파라미터를  과하게 움직이는 것을 방지하여 학습 안정성 확보\n",
    "        optimizer.step() # 클리핑된 그래디언트를 이용해 가중치를 1번 갱신\n",
    "        lr_scheduler.step() # 스케줄러가 현재 스텝 번호에 맞춰 학습률을 선형 감소. 워밍업 등 설정한 전략에 따라 업데이트\n",
    "        train_loss += loss.item() # tensor.item(): PyTorch 텐서 안에 값이 단 하나만 있을 때 그 값을 파이썬 스칼라(숫자 타입)로 꺼내 주는 메서드\n",
    "\n",
    "    # ----- Validation -----\n",
    "    model.eval()\n",
    "    val_loss, preds, labels = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"[Epoch {epoch}] Val  \", leave=False): \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels.extend(batch[\"labels\"].cpu().numpy()) # 이번 배치의 labels 텐서를 다시 CPU로 옮겨 numpy 배열로 변환한 뒤, labels 리스트에 이어 붙여 전체 검증 데이터의 정답 라벨을 모은다\n",
    "            outputs = model(**batch) # 로짓과 loss 반환\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds.extend(torch.argmax(outputs.logits, 1).cpu().numpy()) # 로짓에서 argmax(가장 큰 점수 -> 예측 클래스)를 구한 뒤 리스트에 이어 붙여 전체 검증 데이터의 모든 예측을 모은다\n",
    "\n",
    "    val_acc =. accuracy_score(labels, preds) # accuracy_score: 정확도 계산 함수\n",
    "    print(f\"Epoch {epoch} | Train {train_loss/len(train_loader):.4f} | \"\n",
    "          f\"Val {val_loss/len(val_loader):.4f} | Acc {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        # 가중치만 저장\n",
    "        torch.save(model.state_dict(), \"best_model.h5\") # model.state_dict(): 모델의 모든 학습 가능한 파라미터(가중치, 바이어스)를 파이썬 dict 형태로 반환(키: 레이어 이름 / 값: torch.Tensor(실제 수치 데이터))\n",
    "        # 모델 객체 통째 저장\n",
    "        # torch.save(model, \"best_model_full.h5\")\n",
    "        print(f\"모델 저장 (acc={best_acc:.4f})\")\n",
    "        pat_cnt = 0\n",
    "    else:\n",
    "        pat_cnt += 1\n",
    "        if pat_cnt >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "print(f\"최고 검증 정확도: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65126e71-4a10-46f8-a9d3-c9b3ee6bf94c",
   "metadata": {},
   "source": [
    "### 9) 테스트 예측 & CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd482a54-8c8c-4e88-b3bc-145ce2b8cdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_with_pred.csv 저장 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# best_model.h5 파일에서 state-dict(파라미터 텐서들의 딕셔너리)를 읽어 온다\n",
    "model.load_state_dict(torch.load(\"best_model.h5\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# when 모델 객체 전부 다운했을 때 load 코드\n",
    "# model_full = torch.load(\"best_model_full.pt\", map_location=device)\n",
    "# model.eval()\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predict\", leave=False):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits = model(**batch).logits\n",
    "        all_preds.extend(torch.argmax(logits, 1).cpu().numpy()) # 각 샘플에서 가장 큰 로짓 인덱스(=에측 클래스 ID)를 추출 -> numpy 배열로 변환해 all_preds에 이어 붙임\n",
    "\n",
    "test_df[\"predict\"] = le.inverse_transform(all_preds) # 학습 단계에서 사용한 LabelEncoder (le) 역변환 / 정수 ID -> 원본 문자 라벨\n",
    "test_df.to_csv(\"test_with_pred.csv\", index=False)\n",
    "print(\"test_with_pred.csv 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f74514-8632-411e-957d-5b6411e48d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
